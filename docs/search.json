[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am a master student of Singapore Management University majoring in IT in Business with AI track. I am passionate about AI. I have a strong background in computer science and big data. I am proficient in Python. I have experience in machine learning, deep learning. I am a quick learner and a good team player. I like to learn new things summary and show them with each other. Currently I’m looking for a full-time job in AI field."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\nSingapore Management University | Singapre Master of IT in Business(AI track) | Sept 2023 -\nUniversity of Wollongong | Computer Science B.S in Big Data | March 2020 - March 2023"
  },
  {
    "objectID": "about.html#technical-skills",
    "href": "about.html#technical-skills",
    "title": "About Me",
    "section": "Technical Skills",
    "text": "Technical Skills\n\nMachine Learning & Deep Learning\nComputer Vision\nNatural Language Processing\nReinforcement Learning\nMeta Learning\nGenerative Model\nUnsupervised Learning\nLarge Language Model\nConvex Optmization\nProbabilitic Graphical Model\nDeep Graph Learning"
  },
  {
    "objectID": "about.html#projects",
    "href": "about.html#projects",
    "title": "About Me",
    "section": "Projects",
    "text": "Projects"
  },
  {
    "objectID": "posts/Courses.html",
    "href": "posts/Courses.html",
    "title": "All Courses lead to AI Expert",
    "section": "",
    "text": "Course Resource: https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/\nCourse Video: https://www.youtube.com/playlist?list=PL49CF3715CB9EF31D\nGilbert Strang is the Best Professor in the world and one of the most recognized names in the mathematics. This Linear Algebra courses build from basic to advance. This course is the best math course I have taken.\n\n\n\nCourse Resource: https://ocw.mit.edu/courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/\nCourse Video: https://www.youtube.com/playlist?list=PLUl4u3cNGP63oMNUHXqIUcrkS2PivhN3k\nThis course is another course of the Gilbert Strang, which is build on the [MIT 18.06](https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/). It first review some basic concepts of the Linear Algebra, such as Eigenvalues & EigenVectors, SVD, PCA. Then it introduce some numerical linear algebra methods to solve the linear equations. After that, he introduce the Low-Rank Approximation methods, which are important in current LLM, using to fine-tuning the models. In the end, it introduce the optimization algorithms such as gradient descent and stochastic gradient descent, which are basic idea in the optimization in the neural network.\nThis course is important, after harness those tools and algorithms, you can easily understand the complex equations in the AI papers.\n\n\n\nMIT 18.01 Course Website: https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/\nMIT 18.01 Course Video: https://www.youtube.com/playlist?list=PL590CCC2BC5AF3BC1\nMIT 18.02 Course Website: https://ocw.mit.edu/courses/18-02-multivariable-calculus-fall-2007/\nMIT 18.02 Couse Video: https://www.youtube.com/playlist?list=PL4C4C8A7D06566F38\nCalculus are important in the optimization algorithms. Those two course will build the foundation for the further optimization algorithms. The concept of Jacobian Matrix, Hessian Matrix, Chain Rule are important.\n\n\n\nCourse Website: https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/\nCourse Video: https://www.youtube.com/playlist?list=PLUl4u3cNGP62EaLLH92E_VCN4izBKK6OE\nThe core concept of this course is to view the derivative as linearization. It introduce how to get the first-order and second-order derivative from different function. It also introduce how to calculate the gradient of a functional and stochastic functions. Get the derivative of the random function is important in the training some model. The two method:\n\nRe-parameterized trick/pathwise is the way to get the derivative of the continuous function. The most knowing application is the Variational Autoencoder\nLikelikehood ratio estimator is used to estimate the derivative of the discrete function. The widely known application is the REINFORCE algorithm in the Reinforcement Learning algorithm."
  },
  {
    "objectID": "posts/Courses.html#linear-algebra-mit-18.06",
    "href": "posts/Courses.html#linear-algebra-mit-18.06",
    "title": "All Courses lead to AI Expert",
    "section": "",
    "text": "Course Resource: https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/\nCourse Video: https://www.youtube.com/playlist?list=PL49CF3715CB9EF31D\nGilbert Strang is the Best Professor in the world and one of the most recognized names in the mathematics. This Linear Algebra courses build from basic to advance. This course is the best math course I have taken."
  },
  {
    "objectID": "posts/Courses.html#matrix-methods-from-data-mit-18.065",
    "href": "posts/Courses.html#matrix-methods-from-data-mit-18.065",
    "title": "All Courses lead to AI Expert",
    "section": "",
    "text": "Course Resource: https://ocw.mit.edu/courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/\nCourse Video: https://www.youtube.com/playlist?list=PLUl4u3cNGP63oMNUHXqIUcrkS2PivhN3k\nThis course is another course of the Gilbert Strang, which is build on the [MIT 18.06](https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/). It first review some basic concepts of the Linear Algebra, such as Eigenvalues & EigenVectors, SVD, PCA. Then it introduce some numerical linear algebra methods to solve the linear equations. After that, he introduce the Low-Rank Approximation methods, which are important in current LLM, using to fine-tuning the models. In the end, it introduce the optimization algorithms such as gradient descent and stochastic gradient descent, which are basic idea in the optimization in the neural network.\nThis course is important, after harness those tools and algorithms, you can easily understand the complex equations in the AI papers."
  },
  {
    "objectID": "posts/Courses.html#calculus-mit-18.01-mit-18.02",
    "href": "posts/Courses.html#calculus-mit-18.01-mit-18.02",
    "title": "All Courses lead to AI Expert",
    "section": "",
    "text": "MIT 18.01 Course Website: https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/\nMIT 18.01 Course Video: https://www.youtube.com/playlist?list=PL590CCC2BC5AF3BC1\nMIT 18.02 Course Website: https://ocw.mit.edu/courses/18-02-multivariable-calculus-fall-2007/\nMIT 18.02 Couse Video: https://www.youtube.com/playlist?list=PL4C4C8A7D06566F38\nCalculus are important in the optimization algorithms. Those two course will build the foundation for the further optimization algorithms. The concept of Jacobian Matrix, Hessian Matrix, Chain Rule are important."
  },
  {
    "objectID": "posts/Courses.html#matrix-calculus-mit-18.s096",
    "href": "posts/Courses.html#matrix-calculus-mit-18.s096",
    "title": "All Courses lead to AI Expert",
    "section": "",
    "text": "Course Website: https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/\nCourse Video: https://www.youtube.com/playlist?list=PLUl4u3cNGP62EaLLH92E_VCN4izBKK6OE\nThe core concept of this course is to view the derivative as linearization. It introduce how to get the first-order and second-order derivative from different function. It also introduce how to calculate the gradient of a functional and stochastic functions. Get the derivative of the random function is important in the training some model. The two method:\n\nRe-parameterized trick/pathwise is the way to get the derivative of the continuous function. The most knowing application is the Variational Autoencoder\nLikelikehood ratio estimator is used to estimate the derivative of the discrete function. The widely known application is the REINFORCE algorithm in the Reinforcement Learning algorithm."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yuyang’s Blog",
    "section": "",
    "text": "Let’s understand Large Language Model from scratch\n\n\n\n\n\n\nGenerative Model\n\n\nOverview\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model. I will also display the pros and cons between different models, and how we can combine those models to get better performance\n\n\n\n\n\nFeb 28, 2025\n\n\n2 min\n\n\n287 words\n\n\n2/27/25, 3:02:33 PM\n\n\n\n\n\n\n\n\n\n\n\n\nWhat a Generative Models?\n\n\n\n\n\n\nGenerative Model\n\n\nOverview\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model. I will also display the pros and cons between different models, and how we can combine those models to get better performance\n\n\n\n\n\nFeb 28, 2025\n\n\n1 min\n\n\n102 words\n\n\n2/27/25, 2:30:59 PM\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Dive into Diffusion Models\n\n\n\n\n\n\nGenerative Model\n\n\nLarge Language Model\n\n\n\nThis blog is my learning notes on the Diffusion Models, which is the state-of-art generative models. This blog will cover that is the diffusion models, how to use diffusion models to generate image, text-to-image, video. It also cover lagnuage diffusion model.\n\n\n\n\n\nFeb 28, 2025\n\n\n1 min\n\n\n44 words\n\n\n2/27/25, 2:16:59 PM\n\n\n\n\n\n\n\n\n\n\n\n\nAll Courses lead to AI Expert\n\n\n\n\n\nIn this blog, I am going to summary all the AI courses I have taken during my Master Degree\n\n\n\n\n\nFeb 28, 2025\n\n\n2 min\n\n\n333 words\n\n\n2/8/25, 1:37:40 AM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Generative Model/Diffusion Model.html",
    "href": "posts/Generative Model/Diffusion Model.html",
    "title": "Deep Dive into Diffusion Models",
    "section": "",
    "text": "What is the Diffusion Models?\nDiffusion Model is know\n\n\nDiffusion Model for Discrete Data\nRecently, there are some good exploration for the diffusion model when apply on the. For example, Inception is the first commercial-scale diffusion language model. Which is is faster than the general language model."
  },
  {
    "objectID": "posts/Generative Model/Generative Model Overview.html",
    "href": "posts/Generative Model/Generative Model Overview.html",
    "title": "What a Generative Models?",
    "section": "",
    "text": "What is the Generative Models and Generative AI?\nGenerative models, as the name indicated, are models that can generative new content. Unlike discriminate models, the generative models are sometime hard to train. But why we need generative models in the first place? We want generative models because:\n\nDensity Estimation:\n\n\n\nDensity Estimation is the type of task that\n\n\nConclusion\nIn the blog, we has explore go through several generative models. We explore why we need generative models. For different purposes, we can different choice of the models. On the other hand, we can combine different models to get better performance. There are still more room for the generative models."
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#word-level",
    "href": "posts/LLM/LLM-Overview.html#word-level",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Word Level",
    "text": "Word Level"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#byte-pair-encoding",
    "href": "posts/LLM/LLM-Overview.html#byte-pair-encoding",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Byte-Pair-Encoding",
    "text": "Byte-Pair-Encoding"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#bit-encoding",
    "href": "posts/LLM/LLM-Overview.html#bit-encoding",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Bit Encoding",
    "text": "Bit Encoding"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#embedding",
    "href": "posts/LLM/LLM-Overview.html#embedding",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Embedding",
    "text": "Embedding\nEmbedding is"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#position-encoding",
    "href": "posts/LLM/LLM-Overview.html#position-encoding",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Position Encoding",
    "text": "Position Encoding"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#normalization",
    "href": "posts/LLM/LLM-Overview.html#normalization",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Normalization",
    "text": "Normalization"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#layer-normalization",
    "href": "posts/LLM/LLM-Overview.html#layer-normalization",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Layer Normalization",
    "text": "Layer Normalization"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#rms-normalization",
    "href": "posts/LLM/LLM-Overview.html#rms-normalization",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "RMS Normalization",
    "text": "RMS Normalization\n\nPost-Norm vs. Pre-Norm"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#attention-mechnism",
    "href": "posts/LLM/LLM-Overview.html#attention-mechnism",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Attention Mechnism",
    "text": "Attention Mechnism"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#feed-forward-network",
    "href": "posts/LLM/LLM-Overview.html#feed-forward-network",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Feed Forward Network",
    "text": "Feed Forward Network\n\nMixture of Expert"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#cross-entropy-loss",
    "href": "posts/LLM/LLM-Overview.html#cross-entropy-loss",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Cross Entropy Loss",
    "text": "Cross Entropy Loss"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#model-initilization",
    "href": "posts/LLM/LLM-Overview.html#model-initilization",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Model Initilization",
    "text": "Model Initilization"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#optimizer",
    "href": "posts/LLM/LLM-Overview.html#optimizer",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Optimizer",
    "text": "Optimizer"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#about-gradients",
    "href": "posts/LLM/LLM-Overview.html#about-gradients",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "About Gradients",
    "text": "About Gradients\n\nGradient Accumulations\n\n\nGradiant Clipping"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#mixed-precision",
    "href": "posts/LLM/LLM-Overview.html#mixed-precision",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Mixed Precision",
    "text": "Mixed Precision"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#parallellism",
    "href": "posts/LLM/LLM-Overview.html#parallellism",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Parallellism",
    "text": "Parallellism"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#supervised-fine-tuning",
    "href": "posts/LLM/LLM-Overview.html#supervised-fine-tuning",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Supervised-Fine-Tuning",
    "text": "Supervised-Fine-Tuning"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#reinforcement-learning-from-human-feedback",
    "href": "posts/LLM/LLM-Overview.html#reinforcement-learning-from-human-feedback",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Reinforcement Learning from Human Feedback",
    "text": "Reinforcement Learning from Human Feedback\n\nPPO\n\n\nDPO\n\n\nGRPO"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#quantization",
    "href": "posts/LLM/LLM-Overview.html#quantization",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Quantization",
    "text": "Quantization"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#knowledge-distillation",
    "href": "posts/LLM/LLM-Overview.html#knowledge-distillation",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Knowledge Distillation",
    "text": "Knowledge Distillation"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#prompt-enginnering",
    "href": "posts/LLM/LLM-Overview.html#prompt-enginnering",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Prompt Enginnering",
    "text": "Prompt Enginnering"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#prefix-tuning",
    "href": "posts/LLM/LLM-Overview.html#prefix-tuning",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Prefix-Tuning",
    "text": "Prefix-Tuning"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#adapter",
    "href": "posts/LLM/LLM-Overview.html#adapter",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Adapter",
    "text": "Adapter\n\nLoRA\n\n\nQ-LoRA"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#chatbot",
    "href": "posts/LLM/LLM-Overview.html#chatbot",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "ChatBot",
    "text": "ChatBot\nMost know ChatGPT,"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#ai-agent",
    "href": "posts/LLM/LLM-Overview.html#ai-agent",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "AI Agent",
    "text": "AI Agent"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#llm-as-optimizer",
    "href": "posts/LLM/LLM-Overview.html#llm-as-optimizer",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "LLM as Optimizer",
    "text": "LLM as Optimizer"
  }
]
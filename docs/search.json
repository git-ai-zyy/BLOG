[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am a master student of Singapore Management University majoring in IT in Business with AI track. I am passionate about AI. I have a strong background in computer science and big data. I am proficient in Python. I have experience in machine learning, deep learning. I am a quick learner and a good team player. I like to learn new things summary and show them with each other. Currently I’m looking for a full-time job in AI field."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\nSingapore Management University | Singapre Master of IT in Business(AI track) | Sept 2023 -\nUniversity of Wollongong | Computer Science B.S in Big Data | March 2020 - March 2023"
  },
  {
    "objectID": "about.html#technical-skills",
    "href": "about.html#technical-skills",
    "title": "About Me",
    "section": "Technical Skills",
    "text": "Technical Skills\n\nMachine Learning & Deep Learning\nComputer Vision\nNatural Language Processing\nReinforcement Learning\nMeta Learning\nGenerative Model\nUnsupervised Learning\nLarge Language Model\nConvex Optmization\nProbabilitic Graphical Model\nDeep Graph Learning"
  },
  {
    "objectID": "about.html#projects",
    "href": "about.html#projects",
    "title": "About Me",
    "section": "Projects",
    "text": "Projects"
  },
  {
    "objectID": "posts/Gaussian.html",
    "href": "posts/Gaussian.html",
    "title": "Gaussian Distribution is All you need",
    "section": "",
    "text": "Gaussian Distribution, one of the most important and widely used probability distributions in statistics and machine learning. It is also known as the normal distribution, which is a continuous probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean."
  },
  {
    "objectID": "posts/Generative Model/Generative Model Overview.html",
    "href": "posts/Generative Model/Generative Model Overview.html",
    "title": "What a Generative Models?",
    "section": "",
    "text": "This website contains my notes and thoughts on various topics.\n\nWhat is the Generative Models and Generative AI?\nGenerative models, as the name indicated, are models that can generative new content. Unlike discriminate models, the generative models are sometime hard to train. But why we need generative models in the first place? We want generative models because:\n\nDensity Estimation:\n\n\n\nDensity Estimation is the type of task that\n\n\nConclusion\nIn the blog, we has explore go through several generative models. We explore why we need generative models. For different purposes, we can different choice of the models. On the other hand, we can combine different models to get better performance. There are still more room for the generative models."
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#word-level",
    "href": "posts/LLM/LLM-Overview.html#word-level",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Word Level",
    "text": "Word Level"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#byte-pair-encoding",
    "href": "posts/LLM/LLM-Overview.html#byte-pair-encoding",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Byte-Pair-Encoding",
    "text": "Byte-Pair-Encoding"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#bit-encoding",
    "href": "posts/LLM/LLM-Overview.html#bit-encoding",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Bit Encoding",
    "text": "Bit Encoding"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#embedding",
    "href": "posts/LLM/LLM-Overview.html#embedding",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Embedding",
    "text": "Embedding\nEmbedding is"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#position-encoding",
    "href": "posts/LLM/LLM-Overview.html#position-encoding",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Position Encoding",
    "text": "Position Encoding"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#normalization",
    "href": "posts/LLM/LLM-Overview.html#normalization",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Normalization",
    "text": "Normalization"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#layer-normalization",
    "href": "posts/LLM/LLM-Overview.html#layer-normalization",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Layer Normalization",
    "text": "Layer Normalization"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#rms-normalization",
    "href": "posts/LLM/LLM-Overview.html#rms-normalization",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "RMS Normalization",
    "text": "RMS Normalization\n\nPost-Norm vs. Pre-Norm"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#attention-mechnism",
    "href": "posts/LLM/LLM-Overview.html#attention-mechnism",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Attention Mechnism",
    "text": "Attention Mechnism"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#feed-forward-network",
    "href": "posts/LLM/LLM-Overview.html#feed-forward-network",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Feed Forward Network",
    "text": "Feed Forward Network\n\nMixture of Expert"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#cross-entropy-loss",
    "href": "posts/LLM/LLM-Overview.html#cross-entropy-loss",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Cross Entropy Loss",
    "text": "Cross Entropy Loss"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#model-initilization",
    "href": "posts/LLM/LLM-Overview.html#model-initilization",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Model Initilization",
    "text": "Model Initilization"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#optimizer",
    "href": "posts/LLM/LLM-Overview.html#optimizer",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Optimizer",
    "text": "Optimizer"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#about-gradients",
    "href": "posts/LLM/LLM-Overview.html#about-gradients",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "About Gradients",
    "text": "About Gradients\n\nGradient Accumulations\n\n\nGradiant Clipping"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#mixed-precision",
    "href": "posts/LLM/LLM-Overview.html#mixed-precision",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Mixed Precision",
    "text": "Mixed Precision"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#parallellism",
    "href": "posts/LLM/LLM-Overview.html#parallellism",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Parallellism",
    "text": "Parallellism"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#supervised-fine-tuning",
    "href": "posts/LLM/LLM-Overview.html#supervised-fine-tuning",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Supervised-Fine-Tuning",
    "text": "Supervised-Fine-Tuning"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#reinforcement-learning-from-human-feedback",
    "href": "posts/LLM/LLM-Overview.html#reinforcement-learning-from-human-feedback",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Reinforcement Learning from Human Feedback",
    "text": "Reinforcement Learning from Human Feedback\n\nPPO\n\n\nDPO\n\n\nGRPO"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#quantization",
    "href": "posts/LLM/LLM-Overview.html#quantization",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Quantization",
    "text": "Quantization"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#knowledge-distillation",
    "href": "posts/LLM/LLM-Overview.html#knowledge-distillation",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Knowledge Distillation",
    "text": "Knowledge Distillation"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#prompt-enginnering",
    "href": "posts/LLM/LLM-Overview.html#prompt-enginnering",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Prompt Enginnering",
    "text": "Prompt Enginnering"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#prefix-tuning",
    "href": "posts/LLM/LLM-Overview.html#prefix-tuning",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Prefix-Tuning",
    "text": "Prefix-Tuning"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#adapter",
    "href": "posts/LLM/LLM-Overview.html#adapter",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "Adapter",
    "text": "Adapter\n\nLoRA\n\n\nQ-LoRA"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#chatbot",
    "href": "posts/LLM/LLM-Overview.html#chatbot",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "ChatBot",
    "text": "ChatBot\nMost know ChatGPT,"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#ai-agent",
    "href": "posts/LLM/LLM-Overview.html#ai-agent",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "AI Agent",
    "text": "AI Agent"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#llm-as-optimizer",
    "href": "posts/LLM/LLM-Overview.html#llm-as-optimizer",
    "title": "Let’s understand Large Language Model from scratch",
    "section": "LLM as Optimizer",
    "text": "LLM as Optimizer"
  },
  {
    "objectID": "posts/Generative Model/Diffusion Model.html",
    "href": "posts/Generative Model/Diffusion Model.html",
    "title": "Deep Dive into Diffusion Models",
    "section": "",
    "text": "What is the Diffusion Models?\nDiffusion Model is know\n\n\nDiffusion Model for Discrete Data\nRecently, there are some good exploration for the diffusion model when apply on the. For example, Inception is the first commercial-scale diffusion language model. Which is is faster than the general language model."
  },
  {
    "objectID": "posts/MathToolBox.html",
    "href": "posts/MathToolBox.html",
    "title": "Math Toolbox for AI",
    "section": "",
    "text": "Update\n\n\n\nAdd Vector Normalization\nMathematics is the important and the most confusion part of artificial intelligence. You might think AI is all about fancy neural networks and mind-blowing predictions, but behind every impressive AI breakthrough is a solid foundation of math. Without math, AI is just a bunch of random code guessing its way through problems like a lost tourist with no GPS.\nSo why am I writing this blog? Because AI is everywhere—from recommending your next Netflix binge to predicting stock market trends. Yet, many aspiring AI enthusiasts underestimate the power of mathematics. If you’ve ever wondered, “Do I really need to know linear algebra and calculus to work with AI?” The answer is a resounding YES! And I’m here to make it a little less intimidating (and hopefully a bit more fun).\nNow, let’s talk about the real struggle: the fancy names. When you first start learning AI, it feels like math is just out to confuse you. One moment you’re fine with basic addition, and the next, you’re drowning in terms like “eigenvalues,” “Jacobian matrices,” and “Markov chains.” You hear about “gradient descent” and think, “Oh, that sounds cool,” only to realize it involves partial derivatives and a whole lot of Greek letters that make your head spin. And don’t even get me started on “Lagrange multipliers”—it sounds like something from a sci-fi movie, but turns out to be just another way math likes to keep us on our toes.\nThis blog is about the some keep concept in the mathematics and how they used in the AI. Each part is self-contains, readers can choose parts most interested them."
  },
  {
    "objectID": "posts/MathToolBox.html#norm-of-the-vectorslp-norm",
    "href": "posts/MathToolBox.html#norm-of-the-vectorslp-norm",
    "title": "Math Toolbox for AI",
    "section": "1. Norm of the Vectors(Lp-Norm)",
    "text": "1. Norm of the Vectors(Lp-Norm)\n\n\n\n\n\n\nCaution\n\n\n\nMany people confuse normalization with the length (norm) of a vector, but they are fundamentally different. The Normalization is defined as:\n\\[\nv_{\\text{normlized}} = \\frac{v}{\\| v\\|_p}\n\\]\nwhere \\(\\| v \\|_p\\) is the Norm of the vector, this form is called Lp-Norm.\nNormalization is the process of rescaling a vector so that its norm (magnitude) becomes 1, while preserving its direction. It ensures that all vectors in a dataset have the same scale, which is crucial for numerical stability and model performance in machine learning and deep learning.\n\n\nThe Lp-Norm is a generalization of different norms, including L1-Norm, L2-Norm, and others. It measures the magnitude of a vector in various ways depending on the value of \\(p\\) . Lp-Norm is widely used in machine learning, deep learning, and signal processing for tasks like feature scaling, similarity measurements, and optimization. Mathematically, it defined as:\n\\[\n\\| v\\|_p = (\\sum_i^d |v_i|^p)^{1 / p}\n\\]\nwhere:\n\n\\(p\\): is a positive real number\n\\(|v_i|\\) represents the absolute value of each component of the vectors\n\nThere are some special case of the"
  },
  {
    "objectID": "posts/MathToolBox.html#vector-space",
    "href": "posts/MathToolBox.html#vector-space",
    "title": "Math Toolbox for AI",
    "section": "2. Vector Space",
    "text": "2. Vector Space\n\nHilbert Sapce"
  },
  {
    "objectID": "posts/MathToolBox.html#compare-vectors",
    "href": "posts/MathToolBox.html#compare-vectors",
    "title": "Math Toolbox for AI",
    "section": "3. Compare Vectors",
    "text": "3. Compare Vectors"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "👋🏻Welcome to Yuyang’s Blog",
    "section": "",
    "text": "Gaussian Distribution is All you need\n\n\n\n\n\nAn Toolbox of essential math for artificial intelligence, including linear algebra, probability, calculus, and optimization.\n\n\n\n\n\nMar 3, 2025\n\n\n3 min\n\n\n557 words\n\n\n3/3/25, 7:18:06 PM\n\n\n\n\n\n\n\n\n\n\n\n\nWhat a Generative Models?\n\n\n\n\n\n\nGenerative Model\n\n\nOverview\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model. I will also display the pros and cons between different models, and how we can combine those models to get better performance\n\n\n\n\n\nMar 3, 2025\n\n\n1 min\n\n\n101 words\n\n\n3/3/25, 4:20:11 PM\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Dive into Diffusion Models\n\n\n\n\n\n\nGenerative Model\n\n\nLarge Language Model\n\n\n\nThis blog is my learning notes on the Diffusion Models, which is the state-of-art generative models. This blog will cover that is the diffusion models, how to use diffusion models to generate image, text-to-image, video. It also cover lagnuage diffusion model.\n\n\n\n\n\nMar 3, 2025\n\n\n1 min\n\n\n44 words\n\n\n3/3/25, 4:20:04 PM\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s understand Large Language Model from scratch\n\n\n\n\n\n\nGenerative Model\n\n\nOverview\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model. I will also display the pros and cons between different models, and how we can combine those models to get better performance\n\n\n\n\n\nMar 3, 2025\n\n\n2 min\n\n\n287 words\n\n\n3/3/25, 4:18:36 PM\n\n\n\n\n\n\n\n\n\n\n\n\nMath Toolbox for AI\n\n\n\n\n\nAn Toolbox of essential math for artificial intelligence, including linear algebra, probability, calculus, and optimization.\n\n\n\n\n\nMar 3, 2025\n\n\n3 min\n\n\n481 words\n\n\n3/3/25, 3:33:14 PM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#description-an-open-source-technical-publishing-system-for-creating-beautiful-articles-websites-blogs-books-slides-and-more.-supports-python-r-julia-and-javascript.",
    "href": "index.html#description-an-open-source-technical-publishing-system-for-creating-beautiful-articles-websites-blogs-books-slides-and-more.-supports-python-r-julia-and-javascript.",
    "title": "Yuyang's Blog",
    "section": "description: An open source technical publishing system for creating beautiful articles, websites, blogs, books, slides, and more. Supports Python, R, Julia, and JavaScript.",
    "text": "description: An open source technical publishing system for creating beautiful articles, websites, blogs, books, slides, and more. Supports Python, R, Julia, and JavaScript."
  },
  {
    "objectID": "posts/Gaussian.html#gaussian-distribution",
    "href": "posts/Gaussian.html#gaussian-distribution",
    "title": "Gaussian Distribution is All you need",
    "section": "Gaussian Distribution",
    "text": "Gaussian Distribution\nGaussian distribution, also know as the Normal Distribution, is defined, for a single real-valued variable \\(x\\) as:\n\\[\n\\mathcal{N}(x | \\mu, \\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{1/2}}\\exp\\left\\{- \\frac{1}{2\\sigma^2} (x - \\mu)^2\\right\\}\n\\tag{1}\\]\nwhere:\n\n\\(\\mu\\) called the mean\n\\(\\sigma^2\\) called the variance\n\n\\(\\sigma\\) called the standard deviation\n\n\\(\\beta = 1/\\sigma^2\\) called the precision."
  },
  {
    "objectID": "posts/Gaussian.html#multivariate-gaussian-distribution",
    "href": "posts/Gaussian.html#multivariate-gaussian-distribution",
    "title": "Gaussian Distribution is All you need",
    "section": "Multivariate Gaussian Distribution",
    "text": "Multivariate Gaussian Distribution\nNow, we consider the \\(D\\)-dimensional \\(\\mathbf{x}\\), this lead to the Multivariate Gaussian, which is defined as:\n\\[\n\\mathcal{N}(\\mathcal{\\mathbf{x}|\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}})= \\frac{1}{(2\\pi)^{D / 2}|\\boldsymbol{\\Sigma}|^{1 / 2}} \\exp\\left\\{  - \\frac{1}{2}  (\\mathbf{x}- \\mathbf{\\mu})^{T} \\boldsymbol{\\Sigma}^{-1}(\\mathbf{x} - \\boldsymbol{\\mu})\\right\\}\n\\tag{2}\\]\nwhere:\n\n\\(\\boldsymbol{\\mu}\\) is the \\(D\\)-dimensional mean vector\n\\(\\boldsymbol{\\Sigma}\\) is the \\(D \\times D\\) covariance matrix - \\(\\det \\boldsymbol{\\Sigma}\\) is the determinant of \\(\\boldsymbol{\\Sigma}\\)\n\\(\\Lambda \\equiv  \\boldsymbol{\\Sigma}^{-1}\\) is the precision matrix."
  },
  {
    "objectID": "posts/Gaussian.html#mixture-of-gaussian",
    "href": "posts/Gaussian.html#mixture-of-gaussian",
    "title": "Gaussian Distribution is All you need",
    "section": "Mixture of Gaussian",
    "text": "Mixture of Gaussian\nMore complexity, when we take the linear combination of the basic distribution of Normal Distribution, we will get Mixture of Gaussian Distribution, which is defined as:\n\\[\np(\\mathbf{x}) = \\sum_{k=1}^{K}\\pi_{k}\\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Sigma}_{k})\n\\tag{3}\\]\nwhere:\n\n\\(\\pi_k\\) called the mixing coefficients, who has constraints that\n\\[\n\\begin{array} &\\sum_{k=1}^K \\pi_k = 1  \\\\ 0 \\leq \\pi_{k} \\leq 1\\end{array}\n\\]\n\\(\\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}{k}, \\boldsymbol{\\Sigma}{k})\\) is called a component of the mixture, has its own \\(\\boldsymbol{\\mu}\\) and \\(\\boldsymbol{\\Sigma}_k\\)"
  },
  {
    "objectID": "posts/Gaussian.html#maximum-likelihood-learning",
    "href": "posts/Gaussian.html#maximum-likelihood-learning",
    "title": "Gaussian Distribution is All you need",
    "section": "Maximum Likelihood Learning",
    "text": "Maximum Likelihood Learning\nThe first methods we introduce is the maximum likelihood learning, which is defined as:\n\\[\n\\max P(\\mathcal{D} | \\mu, \\Sigma)\n\\tag{4}\\]\nThe \\(P(\\mathcal{D} | \\mu, \\Sigma)\\) is called the likelihood of the dataset, as we defined in the question, the data points are i.i.d. so, we can write the likelihood function as:\n\\[\nP(\\mathcal{D} | \\mu, \\Sigma) = \\prod_{n = 1}^N \\mathcal{N}(\\mathbf{x}_n | \\mu, \\Sigma)\n\\tag{5}\\]\n\n\n\n\n\n\nLog-Trick\n\n\n\nIn the practice, because the \\(0 \\leq \\mathcal{N}(\\mathbf{x}_n | \\mu, \\Sigma) \\leq 1\\), when multiplying \\(N\\) small number together, might cause the under-flow problem in the computer. So, we use \\(\\log\\)-form of the function to prevent the under-flow. Because the \\(\\log\\) function is the monomtic function, so, when we maximimzie \\(\\log f\\) is same as \\(\\max f\\).\n\n\nSo, the objective function we want to maximize is:\n\\[\n\\ln P(\\mathcal{D} | \\mu, \\Sigma) = \\sum_{n = 1}^N \\ln\\mathcal{N}(\\mathbf{x}_n | \\mu, \\Sigma)\n\\tag{6}\\]\nHow to maximize the Equation 6. The most intuitive of way is set the derivative of the function with respect to 0.\n\nBias of Maximum Likelihood Learning.\nAs we can see, we used the sample mean to derive the sample variance. Because the sample mean estimated from the dataset \\(\\mathcal{D}\\) , it is not same as the true \\(\\mu\\)."
  },
  {
    "objectID": "posts/Gaussian.html#maxium-a-posteriormap-estimate",
    "href": "posts/Gaussian.html#maxium-a-posteriormap-estimate",
    "title": "Gaussian Distribution is All you need",
    "section": "Maxium A Posterior(MAP) Estimate",
    "text": "Maxium A Posterior(MAP) Estimate"
  }
]
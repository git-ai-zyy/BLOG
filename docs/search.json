[
  {
    "objectID": "resource.html",
    "href": "resource.html",
    "title": "Resources",
    "section": "",
    "text": "THis is is some content about mathematics\n\n\n\n\n\n\n\n\n\n\n\n\n‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è\n\n\n\nMIT‚Äôs 18.01 Single Variable Calculus is a foundational mathematics course covering differential and integral calculus. It focuses on the fundamental concepts of‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è\n\n\n\nMIT‚Äôs 18.02 Multivariable Calculus extends the principles of single-variable calculus to functions of multiple variables. It introduces partial derivatives, *multiple‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è\n\n\n\nMIT‚Äôs 18.06 Linear Algebra is a foundational course that explores the fundamental concepts of vectors, matrices, determinants, eigenvalues, and linear transformations.‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è\n\n\n\nMIT‚Äôs 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning is an applied mathematics course that explores the **use of matrix algebra in data‚Ä¶\n\n\n\nMIT\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è\n\n\n\nMIT‚Äôs 18.S096 Matrix Calculus for Machine Learning and Beyond is an advanced mathematics course designed to provide a deep understanding of matrix calculus and its‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNN\n\n\nBasic\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model.‚Ä¶\n\n\n\nStanford\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNN\n\n\nBasic\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model.‚Ä¶\n\n\n\nStanford\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNN\n\n\nBasic\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model.‚Ä¶\n\n\n\nStanford\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNN\n\n\nBasic\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model.‚Ä¶\n\n\n\nStanford\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "üëãüèªWelcome to Yuyang‚Äôs Blog",
    "section": "",
    "text": "Let‚Äôs understand Large Language Model from scratch\n\n\n\n\n\n\nGenerative Model\n\n\nOverview\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model. I will also display the pros and cons between different models, and how we can combine those models to get better performance\n\n\n\n\n\n2025-03-04\n\n\n2 min\n\n\n257 words\n\n\n2025-03-04\n\n\n\n\n\n\n\n\n\n\n\n\nGaussian Distribution is All you need\n\n\n\n\n\n\nMathematics\n\n\n\nAn Toolbox of essential math for artificial intelligence, including linear algebra, probability, calculus, and optimization.\n\n\n\n\n\n2025-03-04\n\n\n7 min\n\n\n1,205 words\n\n\n2025-03-04\n\n\n\n\n\n\n\n\n\n\n\n\nMath Toolbox for AI\n\n\n\n\n\n\nMathematics\n\n\n\nAn Toolbox of essential math for artificial intelligence, including linear algebra, probability, calculus, and optimization.\n\n\n\n\n\n2025-03-04\n\n\n3 min\n\n\n480 words\n\n\n2025-03-04\n\n\n\n\n\n\n\n\n\n\n\n\nWhat a Generative Models?\n\n\n\n\n\n\nGenerative Model\n\n\nOverview\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model. I will also display the pros and cons between different models, and how we can combine those models to get better performance\n\n\n\n\n\n2025-03-04\n\n\n1 min\n\n\n101 words\n\n\n2025-03-03\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Dive into Diffusion Models\n\n\n\n\n\n\nGenerative Model\n\n\nLarge Language Model\n\n\n\nThis blog is my learning notes on the Diffusion Models, which is the state-of-art generative models. This blog will cover that is the diffusion models, how to use diffusion models to generate image, text-to-image, video. It also cover lagnuage diffusion model.\n\n\n\n\n\n2025-03-04\n\n\n1 min\n\n\n44 words\n\n\n2025-03-03\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/MathToolBox.html",
    "href": "posts/MathToolBox.html",
    "title": "Math Toolbox for AI",
    "section": "",
    "text": "Update\n\n\n\nAdd Vector Normalization\nMathematics is the important and the most confusion part of artificial intelligence. You might think AI is all about fancy neural networks and mind-blowing predictions, but behind every impressive AI breakthrough is a solid foundation of math. Without math, AI is just a bunch of random code guessing its way through problems like a lost tourist with no GPS.\nSo why am I writing this blog? Because AI is everywhere‚Äîfrom recommending your next Netflix binge to predicting stock market trends. Yet, many aspiring AI enthusiasts underestimate the power of mathematics. If you‚Äôve ever wondered, ‚ÄúDo I really need to know linear algebra and calculus to work with AI?‚Äù The answer is a resounding YES! And I‚Äôm here to make it a little less intimidating (and hopefully a bit more fun).\nNow, let‚Äôs talk about the real struggle: the fancy names. When you first start learning AI, it feels like math is just out to confuse you. One moment you‚Äôre fine with basic addition, and the next, you‚Äôre drowning in terms like ‚Äúeigenvalues,‚Äù ‚ÄúJacobian matrices,‚Äù and ‚ÄúMarkov chains.‚Äù You hear about ‚Äúgradient descent‚Äù and think, ‚ÄúOh, that sounds cool,‚Äù only to realize it involves partial derivatives and a whole lot of Greek letters that make your head spin. And don‚Äôt even get me started on ‚ÄúLagrange multipliers‚Äù‚Äîit sounds like something from a sci-fi movie, but turns out to be just another way math likes to keep us on our toes.\nThis blog is about the some keep concept in the mathematics and how they used in the AI. Each part is self-contains, readers can choose parts most interested them."
  },
  {
    "objectID": "posts/MathToolBox.html#norm-of-the-vectorslp-norm",
    "href": "posts/MathToolBox.html#norm-of-the-vectorslp-norm",
    "title": "Math Toolbox for AI",
    "section": "1. Norm of the Vectors(Lp-Norm)",
    "text": "1. Norm of the Vectors(Lp-Norm)\n\n\n\n\n\n\nCaution\n\n\n\nMany people confuse normalization with the length (norm) of a vector, but they are fundamentally different. The Normalization is defined as:\n\\[\nv_{\\text{normlized}} = \\frac{v}{\\| v\\|_p}\n\\]\nwhere \\(\\| v \\|_p\\) is the Norm of the vector, this form is called Lp-Norm.\nNormalization is the process of rescaling a vector so that its norm (magnitude) becomes 1, while preserving its direction. It ensures that all vectors in a dataset have the same scale, which is crucial for numerical stability and model performance in machine learning and deep learning.\n\n\nThe Lp-Norm is a generalization of different norms, including L1-Norm, L2-Norm, and others. It measures the magnitude of a vector in various ways depending on the value of¬†\\(p\\) . Lp-Norm is widely used in machine learning, deep learning, and signal processing for tasks like feature scaling, similarity measurements, and optimization. Mathematically, it defined as:\n\\[\n\\| v\\|_p = (\\sum_i^d |v_i|^p)^{1 / p}\n\\]\nwhere:\n\n\\(p\\): is a positive real number\n\\(|v_i|\\) represents the absolute value of each component of the vectors\n\nThere are some special case of the"
  },
  {
    "objectID": "posts/MathToolBox.html#vector-space",
    "href": "posts/MathToolBox.html#vector-space",
    "title": "Math Toolbox for AI",
    "section": "2. Vector Space",
    "text": "2. Vector Space\n\nHilbert Sapce"
  },
  {
    "objectID": "posts/MathToolBox.html#compare-vectors",
    "href": "posts/MathToolBox.html#compare-vectors",
    "title": "Math Toolbox for AI",
    "section": "3. Compare Vectors",
    "text": "3. Compare Vectors"
  },
  {
    "objectID": "posts/Generative Model/Diffusion Model.html",
    "href": "posts/Generative Model/Diffusion Model.html",
    "title": "Deep Dive into Diffusion Models",
    "section": "",
    "text": "What is the Diffusion Models?\nDiffusion Model is know\n\n\nDiffusion Model for Discrete Data\nRecently, there are some good exploration for the diffusion model when apply on the. For example, Inception is the first commercial-scale diffusion language model. Which is is faster than the general language model."
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#word-level",
    "href": "posts/LLM/LLM-Overview.html#word-level",
    "title": "Let‚Äôs understand Large Language Model from scratch",
    "section": "Word Level",
    "text": "Word Level"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#byte-pair-encoding",
    "href": "posts/LLM/LLM-Overview.html#byte-pair-encoding",
    "title": "Let‚Äôs understand Large Language Model from scratch",
    "section": "Byte-Pair-Encoding",
    "text": "Byte-Pair-Encoding"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#bit-encoding",
    "href": "posts/LLM/LLM-Overview.html#bit-encoding",
    "title": "Let‚Äôs understand Large Language Model from scratch",
    "section": "Bit Encoding",
    "text": "Bit Encoding"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#embedding",
    "href": "posts/LLM/LLM-Overview.html#embedding",
    "title": "Let‚Äôs understand Large Language Model from scratch",
    "section": "Embedding",
    "text": "Embedding\nEmbedding is"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#position-encoding",
    "href": "posts/LLM/LLM-Overview.html#position-encoding",
    "title": "Let‚Äôs understand Large Language Model from scratch",
    "section": "Position Encoding",
    "text": "Position Encoding"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#normalization",
    "href": "posts/LLM/LLM-Overview.html#normalization",
    "title": "Let‚Äôs understand Large Language Model from scratch",
    "section": "Normalization",
    "text": "Normalization"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#layer-normalization",
    "href": "posts/LLM/LLM-Overview.html#layer-normalization",
    "title": "Let‚Äôs understand Large Language Model from scratch",
    "section": "Layer Normalization",
    "text": "Layer Normalization"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#rms-normalization",
    "href": "posts/LLM/LLM-Overview.html#rms-normalization",
    "title": "Let‚Äôs understand Large Language Model from scratch",
    "section": "RMS Normalization",
    "text": "RMS Normalization\n\nPost-Norm vs.¬†Pre-Norm"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#attention-mechnism",
    "href": "posts/LLM/LLM-Overview.html#attention-mechnism",
    "title": "Let‚Äôs understand Large Language Model from scratch",
    "section": "Attention Mechnism",
    "text": "Attention Mechnism"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#feed-forward-network",
    "href": "posts/LLM/LLM-Overview.html#feed-forward-network",
    "title": "Let‚Äôs understand Large Language Model from scratch",
    "section": "Feed Forward Network",
    "text": "Feed Forward Network\n\nMixture of Expert"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#cross-entropy-loss",
    "href": "posts/LLM/LLM-Overview.html#cross-entropy-loss",
    "title": "Let‚Äôs understand Large Language Model from scratch",
    "section": "Cross Entropy Loss",
    "text": "Cross Entropy Loss"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#model-initilization",
    "href": "posts/LLM/LLM-Overview.html#model-initilization",
    "title": "Let‚Äôs understand Large Language Model from scratch",
    "section": "Model Initilization",
    "text": "Model Initilization"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#optimizer",
    "href": "posts/LLM/LLM-Overview.html#optimizer",
    "title": "Let‚Äôs understand Large Language Model from scratch",
    "section": "Optimizer",
    "text": "Optimizer"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#about-gradients",
    "href": "posts/LLM/LLM-Overview.html#about-gradients",
    "title": "Let‚Äôs understand Large Language Model from scratch",
    "section": "About Gradients",
    "text": "About Gradients\n\nGradient Accumulations\n\n\nGradiant Clipping"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#mixed-precision",
    "href": "posts/LLM/LLM-Overview.html#mixed-precision",
    "title": "Let‚Äôs understand Large Language Model from scratch",
    "section": "Mixed Precision",
    "text": "Mixed Precision"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#parallellism",
    "href": "posts/LLM/LLM-Overview.html#parallellism",
    "title": "Let‚Äôs understand Large Language Model from scratch",
    "section": "Parallellism",
    "text": "Parallellism"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#supervised-fine-tuning",
    "href": "posts/LLM/LLM-Overview.html#supervised-fine-tuning",
    "title": "Let‚Äôs understand Large Language Model from scratch",
    "section": "Supervised-Fine-Tuning",
    "text": "Supervised-Fine-Tuning"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#reinforcement-learning-from-human-feedback",
    "href": "posts/LLM/LLM-Overview.html#reinforcement-learning-from-human-feedback",
    "title": "Let‚Äôs understand Large Language Model from scratch",
    "section": "Reinforcement Learning from Human Feedback",
    "text": "Reinforcement Learning from Human Feedback\n\nPPO\n\n\nDPO\n\n\nGRPO"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#quantization",
    "href": "posts/LLM/LLM-Overview.html#quantization",
    "title": "Let‚Äôs understand Large Language Model from scratch",
    "section": "Quantization",
    "text": "Quantization"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#knowledge-distillation",
    "href": "posts/LLM/LLM-Overview.html#knowledge-distillation",
    "title": "Let‚Äôs understand Large Language Model from scratch",
    "section": "Knowledge Distillation",
    "text": "Knowledge Distillation"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#prompt-enginnering",
    "href": "posts/LLM/LLM-Overview.html#prompt-enginnering",
    "title": "Let‚Äôs understand Large Language Model from scratch",
    "section": "Prompt Enginnering",
    "text": "Prompt Enginnering"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#prefix-tuning",
    "href": "posts/LLM/LLM-Overview.html#prefix-tuning",
    "title": "Let‚Äôs understand Large Language Model from scratch",
    "section": "Prefix-Tuning",
    "text": "Prefix-Tuning"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#adapter",
    "href": "posts/LLM/LLM-Overview.html#adapter",
    "title": "Let‚Äôs understand Large Language Model from scratch",
    "section": "Adapter",
    "text": "Adapter\n\nLoRA\n\n\nQ-LoRA"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#chatbot",
    "href": "posts/LLM/LLM-Overview.html#chatbot",
    "title": "Let‚Äôs understand Large Language Model from scratch",
    "section": "ChatBot",
    "text": "ChatBot\nMost know ChatGPT,"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#ai-agent",
    "href": "posts/LLM/LLM-Overview.html#ai-agent",
    "title": "Let‚Äôs understand Large Language Model from scratch",
    "section": "AI Agent",
    "text": "AI Agent"
  },
  {
    "objectID": "posts/LLM/LLM-Overview.html#llm-as-optimizer",
    "href": "posts/LLM/LLM-Overview.html#llm-as-optimizer",
    "title": "Let‚Äôs understand Large Language Model from scratch",
    "section": "LLM as Optimizer",
    "text": "LLM as Optimizer"
  },
  {
    "objectID": "posts/Generative Model/Generative Model Overview.html",
    "href": "posts/Generative Model/Generative Model Overview.html",
    "title": "What a Generative Models?",
    "section": "",
    "text": "What is the Generative Models and Generative AI?\nGenerative models, as the name indicated, are models that can generative new content. Unlike discriminate models, the generative models are sometime hard to train. But why we need generative models in the first place? We want generative models because:\n\nDensity Estimation:\n\n\n\nDensity Estimation is the type of task that\n\n\nConclusion\nIn the blog, we has explore go through several generative models. We explore why we need generative models. For different purposes, we can different choice of the models. On the other hand, we can combine different models to get better performance. There are still more room for the generative models."
  },
  {
    "objectID": "posts/Gaussian.html",
    "href": "posts/Gaussian.html",
    "title": "Gaussian Distribution is All you need",
    "section": "",
    "text": "Gaussian Distribution, one of the most important and widely used probability distributions in statistics and machine learning. It is also known as the normal distribution, which is a continuous probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean. In the blog, I will walk through the AI field from basic normal distribution, and see how this tree spread across the world."
  },
  {
    "objectID": "posts/Gaussian.html#gaussian-distribution",
    "href": "posts/Gaussian.html#gaussian-distribution",
    "title": "Gaussian Distribution is All you need",
    "section": "Gaussian Distribution",
    "text": "Gaussian Distribution\nGaussian distribution, also know as the Normal Distribution, is defined, for a single real-valued variable \\(x\\) as:\n\\[\n\\mathcal{N}(x | \\mu, \\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{1/2}}\\exp\\left\\{- \\frac{1}{2\\sigma^2} (x - \\mu)^2\\right\\}\n\\tag{1}\\]\nwhere:\n\n\\(\\mu\\) called the mean\n\\(\\sigma^2\\) called the variance\n\n\\(\\sigma\\) called the standard deviation\n\n\\(\\beta = 1/\\sigma^2\\) called the precision."
  },
  {
    "objectID": "posts/Gaussian.html#multivariate-gaussian-distribution",
    "href": "posts/Gaussian.html#multivariate-gaussian-distribution",
    "title": "Gaussian Distribution is All you need",
    "section": "Multivariate Gaussian Distribution",
    "text": "Multivariate Gaussian Distribution\nNow, we consider the \\(D\\)-dimensional \\(\\mathbf{x}\\), this lead to the Multivariate Gaussian, which is defined as:\n\\[\n\\mathcal{N}(\\mathcal{\\mathbf{x}|\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}})= \\frac{1}{(2\\pi)^{D / 2}|\\boldsymbol{\\Sigma}|^{1 / 2}} \\exp\\left\\{  - \\frac{1}{2}  (\\mathbf{x}- \\mathbf{\\mu})^{T} \\boldsymbol{\\Sigma}^{-1}(\\mathbf{x} - \\boldsymbol{\\mu})\\right\\}\n\\tag{2}\\]\nwhere:\n\n\\(\\boldsymbol{\\mu}\\) is the \\(D\\)-dimensional mean vector\n\\(\\boldsymbol{\\Sigma}\\) is the \\(D \\times D\\) covariance matrix - \\(\\det \\boldsymbol{\\Sigma}\\) is the determinant of \\(\\boldsymbol{\\Sigma}\\)\n\\(\\Lambda \\equiv  \\boldsymbol{\\Sigma}^{-1}\\) is the precision matrix."
  },
  {
    "objectID": "posts/Gaussian.html#mixture-of-gaussian",
    "href": "posts/Gaussian.html#mixture-of-gaussian",
    "title": "Gaussian Distribution is All you need",
    "section": "Mixture of Gaussian",
    "text": "Mixture of Gaussian\nMore complexity, when we take the linear combination of the basic distribution of Normal Distribution, we will get Mixture of Gaussian Distribution, which is defined as:\n\\[\np(\\mathbf{x}) = \\sum_{k=1}^{K}\\pi_{k}\\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Sigma}_{k})\n\\tag{3}\\]\nwhere:\n\n\\(\\pi_k\\) called the mixing coefficients, who has constraints that\n\\[\n\\begin{array} &\\sum_{k=1}^K \\pi_k = 1  \\\\ 0 \\leq \\pi_{k} \\leq 1\\end{array}\n\\]\n\\(\\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}{k}, \\boldsymbol{\\Sigma}{k})\\) is called a component of the mixture, has its own \\(\\boldsymbol{\\mu}\\) and \\(\\boldsymbol{\\Sigma}_k\\)"
  },
  {
    "objectID": "posts/Gaussian.html#linear-gaussian-model",
    "href": "posts/Gaussian.html#linear-gaussian-model",
    "title": "Gaussian Distribution is All you need",
    "section": "Linear Gaussian Model",
    "text": "Linear Gaussian Model"
  },
  {
    "objectID": "posts/Gaussian.html#gaussian-in-high-dimension",
    "href": "posts/Gaussian.html#gaussian-in-high-dimension",
    "title": "Gaussian Distribution is All you need",
    "section": "Gaussian In High-Dimension",
    "text": "Gaussian In High-Dimension"
  },
  {
    "objectID": "posts/Gaussian.html#maximum-likelihood-learning",
    "href": "posts/Gaussian.html#maximum-likelihood-learning",
    "title": "Gaussian Distribution is All you need",
    "section": "Maximum Likelihood Learning",
    "text": "Maximum Likelihood Learning\nThe first methods we introduce is the maximum likelihood learning, which is defined as:\n\\[\n\\max P(\\mathcal{D} | \\mu, \\Sigma)\n\\tag{4}\\]\nThe \\(P(\\mathcal{D} | \\mu, \\Sigma)\\) is called the likelihood of the dataset, as we defined in the question, the data points are i.i.d. so, we can write the likelihood function as:\n\\[\nP(\\mathcal{D} | \\mu, \\Sigma) = \\prod_{n = 1}^N \\mathcal{N}(\\mathbf{x}_n | \\mu, \\Sigma)\n\\tag{5}\\]\n\n\n\n\n\n\nLog Trick\n\n\n\nIn the practice, because the \\(0 \\leq \\mathcal{N}(\\mathbf{x}_n | \\mu, \\Sigma) \\leq 1\\), when multiplying \\(N\\) small number together, might cause the under-flow problem in the computer. So, we use \\(\\log\\)-form of the function to prevent the under-flow. Because the \\(\\log\\) function is the monomtic function, so, when we maximimzie \\(\\log f\\) is same as \\(\\max f\\).\n\n\nSo, the objective function we want to maximize is:\n\\[\n\\ln P(\\mathcal{D} | \\mu, \\Sigma) = \\sum_{n = 1}^N \\ln\\mathcal{N}(\\mathbf{x}_n | \\mu, \\Sigma)\n\\tag{6}\\]\nHow to maximize the Equation¬†6. The most intuitive of way is set the derivative of the function with respect to 0.\n\nGradient Descent\n\n\nBias of Maximum Likelihood Learning.\nAs we can see, we used the sample mean to derive the sample variance. Because the sample mean estimated from the dataset \\(\\mathcal{D}\\) , it is not same as the true \\(\\mu\\)."
  },
  {
    "objectID": "posts/Gaussian.html#maxium-a-posteriormap-estimate",
    "href": "posts/Gaussian.html#maxium-a-posteriormap-estimate",
    "title": "Gaussian Distribution is All you need",
    "section": "Maxium A Posterior(MAP) Estimate",
    "text": "Maxium A Posterior(MAP) Estimate"
  },
  {
    "objectID": "posts/Gaussian.html#expectation-maximizationem-algorithms",
    "href": "posts/Gaussian.html#expectation-maximizationem-algorithms",
    "title": "Gaussian Distribution is All you need",
    "section": "Expectation-Maximization(EM) Algorithms",
    "text": "Expectation-Maximization(EM) Algorithms"
  },
  {
    "objectID": "posts/Gaussian.html#sequential-estimation",
    "href": "posts/Gaussian.html#sequential-estimation",
    "title": "Gaussian Distribution is All you need",
    "section": "Sequential Estimation",
    "text": "Sequential Estimation\nSo far we have assume that we can ‚Äúsee‚Äù the whole dataset at once. In practice, we sometime cannot get the whole dataset at one, because:\n\nThe data points comes in sequential, e.g.¬†Online Learning\nThe dataset is too big that can not fit into the memory of the computer.\n\nWe have to way to learn the parameters in sequential version. One of the method is called Robbins-Monro algorithm.\n\nWelford‚Äôs Algorithm\n\nRobbins-Monro Algorithm\n\n\n\nKalman Filter\n\n\nStochastic Gradient Descent\n\n\nExpoential Moving Average(EMA)"
  },
  {
    "objectID": "posts/Gaussian.html#linear-regression",
    "href": "posts/Gaussian.html#linear-regression",
    "title": "Gaussian Distribution is All you need",
    "section": "Linear Regression",
    "text": "Linear Regression"
  },
  {
    "objectID": "posts/Gaussian.html#gaussian-process",
    "href": "posts/Gaussian.html#gaussian-process",
    "title": "Gaussian Distribution is All you need",
    "section": "Gaussian Process",
    "text": "Gaussian Process"
  },
  {
    "objectID": "posts/Gaussian.html#clustering",
    "href": "posts/Gaussian.html#clustering",
    "title": "Gaussian Distribution is All you need",
    "section": "Clustering",
    "text": "Clustering"
  },
  {
    "objectID": "posts/Gaussian.html#deep-learning",
    "href": "posts/Gaussian.html#deep-learning",
    "title": "Gaussian Distribution is All you need",
    "section": "Deep Learning",
    "text": "Deep Learning\n\nModel Initilization\nAs mention in the (He et al. 2015)\n\nLoRA\n(Hu et al. 2021)\n\n\n\nLoRA Image Source:(Hu et al. 2021)\n\n\nIn the LLM, the number of parameters is huge, how we can file-tune those huge parameters. Using parameters efficient fine-tuning technique is one technique. LoRA is one of the PEFT that widely used in the file-tuning huge data.\nThe \\(A\\) matrix is initilization as the Normal DistributionEquation¬†2"
  },
  {
    "objectID": "posts/Gaussian.html#normalization",
    "href": "posts/Gaussian.html#normalization",
    "title": "Gaussian Distribution is All you need",
    "section": "Normalization",
    "text": "Normalization\n\nLayer Normlization\n(Xiong et al. 2020)\n\n\nRMS Normlization\nThis is the RMS with (Zhang and Sennrich, n.d.)"
  },
  {
    "objectID": "posts/Gaussian.html#generative-models",
    "href": "posts/Gaussian.html#generative-models",
    "title": "Gaussian Distribution is All you need",
    "section": "Generative Models",
    "text": "Generative Models\nIn the Generative Models, the Normal Distribution is sometime used as the noise add to the original data, or the prior distribution of some unknown distribution that we are trying to get. In this chapter,"
  },
  {
    "objectID": "posts/Gaussian.html#generative-adversarial-networks",
    "href": "posts/Gaussian.html#generative-adversarial-networks",
    "title": "Gaussian Distribution is All you need",
    "section": "Generative Adversarial Networks",
    "text": "Generative Adversarial Networks\nThe Latent Space input to the generator is of sampled from a Gaussian Distribution (Goodfellow et al. 2014)\n\nVariational Autoencoders(VAE)\nVAEs uses a Gaussian Latent space to learn a generative model of data. (Kingma and Welling 2022)\nA VAE assumes that data is generated from a latent variable \\(Z\\), which follows a Gaussian prior: \\(Z \\sim \\mathcal{N}(0, I)\\). The VAE model learn a probabilistic mapping from \\(Z\\) to the observed data \\(X\\) using:\n\nEncoder: \\(q(Z |X ) \\sim \\mathcal{N}(\\mu_\\theta(X), \\sigma^2_\\theta(X))\\)\nDecoder: \\(P(X | Z)\\) reconstructs \\(X\\) from \\(Z\\)"
  },
  {
    "objectID": "posts/Gaussian.html#diffusion-models",
    "href": "posts/Gaussian.html#diffusion-models",
    "title": "Gaussian Distribution is All you need",
    "section": "Diffusion Models",
    "text": "Diffusion Models\n(For more details, check my this blog about diffusion models)\nThis paper (Ho, Jain, and Abbeel, n.d.)"
  },
  {
    "objectID": "posts/Gaussian.html#reinforcement-learning",
    "href": "posts/Gaussian.html#reinforcement-learning",
    "title": "Gaussian Distribution is All you need",
    "section": "Reinforcement Learning",
    "text": "Reinforcement Learning\nIn the Reinforcement Learning, Gaussian Distribution are commonly used in policy gradient methods like Trust Region Optimization(TRPO) and Proximal Policy Optimization(PPO), where policies are modeled as Gaussian distribution.\nWhen Exploration the environment, the exploration is often handled by Gaussian noise, such as in Deep Deterministc Policy Gradient"
  },
  {
    "objectID": "posts/Gaussian.html#meta-learning",
    "href": "posts/Gaussian.html#meta-learning",
    "title": "Gaussian Distribution is All you need",
    "section": "Meta Learning",
    "text": "Meta Learning\nBayesian Meta Learning: Gaussian priors over parameters enable fast adaptation to new tasks in few-shot learning.\nLatent Task Representation: Many meta-learning frameworks use Gaussian distributions in the latent space to generalize across different tasks efficiently\nUncertainty Estimation: Gaussian-based models in meta-learning help quantify uncertainty, improving robustness in real-world application."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am a master student of Singapore Management University majoring in IT in Business with AI track. I am passionate about AI. I have a strong background in computer science and big data. I am proficient in Python. I have experience in machine learning, deep learning. I am a quick learner and a good team player. I like to learn new things summary and show them with each other. Currently I‚Äôm looking for a full-time job in AI field."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\nSingapore Management University | Singapre Master of IT in Business(AI track) | Sept 2023 -\nUniversity of Wollongong | Computer Science B.S in Big Data | March 2020 - March 2023"
  },
  {
    "objectID": "about.html#technical-skills",
    "href": "about.html#technical-skills",
    "title": "About Me",
    "section": "Technical Skills",
    "text": "Technical Skills\n\nMachine Learning & Deep Learning\nComputer Vision\nNatural Language Processing\nReinforcement Learning\nMeta Learning\nGenerative Model\nUnsupervised Learning\nLarge Language Model\nConvex Optmization\nProbabilitic Graphical Model\nDeep Graph Learning"
  },
  {
    "objectID": "about.html#projects",
    "href": "about.html#projects",
    "title": "About Me",
    "section": "Projects",
    "text": "Projects"
  },
  {
    "objectID": "courses/mit-matrix-method.html",
    "href": "courses/mit-matrix-method.html",
    "title": "MIT 18.065: Matrix Methods in Data Analysis, Signal Processing, and Machine Learning",
    "section": "",
    "text": "Course Link"
  },
  {
    "objectID": "courses/mit-tinyML.html",
    "href": "courses/mit-tinyML.html",
    "title": "MIT 6.5940: TinyML and Efficient Deep Learning Computing",
    "section": "",
    "text": "Course Link"
  },
  {
    "objectID": "courses/ucb-llm-agent.html",
    "href": "courses/ucb-llm-agent.html",
    "title": "UCB CS294/194-196 Large Language Model Agents",
    "section": "",
    "text": "Course Link"
  },
  {
    "objectID": "courses/ucb-llm-agent2.html",
    "href": "courses/ucb-llm-agent2.html",
    "title": "UCB CS294/194-196: Advanced Large Language Model Agents",
    "section": "",
    "text": "Course Link"
  },
  {
    "objectID": "courses/ucb-cs285-drl.html",
    "href": "courses/ucb-cs285-drl.html",
    "title": "UCB CS285: Deep Reinforcement Learning",
    "section": "",
    "text": "Course Link"
  },
  {
    "objectID": "resource.html#lab-reports",
    "href": "resource.html#lab-reports",
    "title": "Resources",
    "section": "",
    "text": "NN\n\n\nBasic\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model.‚Ä¶\n\n\n\nStanford\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNN\n\n\nBasic\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model.‚Ä¶\n\n\n\nStanford\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMathematics\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model.‚Ä¶\n\n\n\nMIT\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resource.html#meeting-notes",
    "href": "resource.html#meeting-notes",
    "title": "Resources",
    "section": "",
    "text": "Mathematics\n\n\n\nAn Toolbox of essential math for artificial intelligence, including linear algebra, probability, calculus, and optimization.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMathematics\n\n\n\nAn Toolbox of essential math for artificial intelligence, including linear algebra, probability, calculus, and optimization.\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "courses/mit-flow-mathcing.html",
    "href": "courses/mit-flow-mathcing.html",
    "title": "MIT 6.S184: Introduction to Flow Matching and Diffusion Models",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/mit-diffusionmodel.html",
    "href": "courses/mit-diffusionmodel.html",
    "title": "MIT 6.S183: A Practical Introduction to Diffusion Models",
    "section": "",
    "text": "Course Link"
  },
  {
    "objectID": "courses/mit-datacentric.html",
    "href": "courses/mit-datacentric.html",
    "title": "MIT Introduction to Data-Centric AI",
    "section": "",
    "text": "Course Link"
  },
  {
    "objectID": "courses/stanford-graph.html",
    "href": "courses/stanford-graph.html",
    "title": "Stanford CS224W: Machine Learning with Graphs",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/stanford-rl.html",
    "href": "courses/stanford-rl.html",
    "title": "Stanford CS234: Reinforcement Learning",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/stanford-meta.html",
    "href": "courses/stanford-meta.html",
    "title": "Stanford CS 330: Deep Multi-Task and Meta Learning",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/cmu-dl.html",
    "href": "courses/cmu-dl.html",
    "title": "CMU 11-785: Introduction to Deep Learning",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/ucb-dl.html",
    "href": "courses/ucb-dl.html",
    "title": "UCB CS182/282A: Designing, Visualizing and Understanding Deep Neural Networks",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/cmu-convexoptimization.html",
    "href": "courses/cmu-convexoptimization.html",
    "title": "CMU 10-725: Convex Optimization",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/cmu-pgm.html",
    "href": "courses/cmu-pgm.html",
    "title": "CMU 10-708: Probabilistic Graphical Models",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/stanford-ml.html",
    "href": "courses/stanford-ml.html",
    "title": "Stanford CS229: Machine Learning",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/hdp.html",
    "href": "courses/hdp.html",
    "title": "High-Dimensional Probability and Applications in Data Science",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/stanford-cv.html",
    "href": "courses/stanford-cv.html",
    "title": "CS231n: Deep Learning for Computer Vision",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/cmu-llms.html",
    "href": "courses/cmu-llms.html",
    "title": "CMU 11-868: Large Language Model Systems",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/mit-aalg.html",
    "href": "courses/mit-aalg.html",
    "title": "MIT 6.854/18.415J: Advanced Algorithms",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/cornell-mlh.html",
    "href": "courses/cornell-mlh.html",
    "title": "CS 5775: Machine Learning Hardware and Systems",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/cmu-dls.html",
    "href": "courses/cmu-dls.html",
    "title": "CMU 11-714: Deep Learning Systems",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/stanford-dgm.html",
    "href": "courses/stanford-dgm.html",
    "title": "Stanford CS236: Deep Generative Models",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/ucb-unsupervised.html",
    "href": "courses/ucb-unsupervised.html",
    "title": "UCB CS294-158: Deep Unsupervised Learning",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/cmu-mml.html",
    "href": "courses/cmu-mml.html",
    "title": "CMU 11-777: MultiModal Machine Learning",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/coursera-dl.html",
    "href": "courses/coursera-dl.html",
    "title": "Coursera: Deep Learning Specialization",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/stanford-prob.html",
    "href": "courses/stanford-prob.html",
    "title": "Stanford CS109: Probability for Computer Scientists",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/ucb-algorithms.html",
    "href": "courses/ucb-algorithms.html",
    "title": "UCB CS170: Efficient Algorithms and Intractable Problems",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/cmu-llm.html",
    "href": "courses/cmu-llm.html",
    "title": "CMU 11-667:Large Language Models: Methods and Applications",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/cmu-anlp.html",
    "href": "courses/cmu-anlp.html",
    "title": "CMU 11-711: Advanced NLP",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/coursera-ml.html",
    "href": "courses/coursera-ml.html",
    "title": "Coursera: Machine Learning Specialization",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/gpu-mode.html",
    "href": "courses/gpu-mode.html",
    "title": "GPU Mode",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/mit-algorithms.html",
    "href": "courses/mit-algorithms.html",
    "title": "MIT 6.006: Introduction To Algorithms",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/cmu-mls.html",
    "href": "courses/cmu-mls.html",
    "title": "CMU 15-849: Machine Learning Systems",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "resource.html#computer-vision",
    "href": "resource.html#computer-vision",
    "title": "Resources",
    "section": "Computer Vision",
    "text": "Computer Vision\n\n\n\n\n\n\n\n\n\n\nCS231n: Deep Learning for Computer Vision\n\n\n\nNN\n\n\nBasic\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model.‚Ä¶\n\n\n\nStanford\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resource.html#natural-language-processing",
    "href": "resource.html#natural-language-processing",
    "title": "Resources",
    "section": "Natural Language Processing",
    "text": "Natural Language Processing\n\n\n\n\n\n\n\n\n\n\nStanford CS224N: Natural Language Processing with Deep Learning\n\n\n\nNN\n\n\nBasic\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model.‚Ä¶\n\n\n\nStanford\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCMU 11-711: Advanced NLP\n\n\n\nNN\n\n\nBasic\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model.‚Ä¶\n\n\n\nStanford\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCMU 11-667:Large Language Models: Methods and Applications\n\n\n\nNN\n\n\nBasic\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model.‚Ä¶\n\n\n\nStanford\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUCB CS294/194-196 Large Language Model Agents\n\n\n\nNN\n\n\nBasic\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model.‚Ä¶\n\n\n\nUCB\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUCB CS294/194-196: Advanced Large Language Model Agents\n\n\n\nNN\n\n\nBasic\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model.‚Ä¶\n\n\n\nUCB\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resource.html#reinforcement-learning-robotics",
    "href": "resource.html#reinforcement-learning-robotics",
    "title": "Resources",
    "section": "Reinforcement Learning & Robotics",
    "text": "Reinforcement Learning & Robotics\n\n\n\n\n\n\n\n\n\n\nStanford CS234: Reinforcement Learning\n\n\n\nNN\n\n\nBasic\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model.‚Ä¶\n\n\n\nStanford\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUCB CS182/282A: Designing, Visualizing and Understanding Deep Neural Networks\n\n\n\nNN\n\n\nBasic\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model.‚Ä¶\n\n\n\nStanford\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resource.html#generative-models",
    "href": "resource.html#generative-models",
    "title": "Resources",
    "section": "Generative Models",
    "text": "Generative Models\n\n\n\n\n\n\n\n\n\n\nStanford CS236: Deep Generative Models\n\n\n\nNN\n\n\nBasic\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model.‚Ä¶\n\n\n\nStanford\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUCB CS294-158: Deep Unsupervised Learning\n\n\n\nNN\n\n\nBasic\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model.‚Ä¶\n\n\n\nStanford\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resource.html#others",
    "href": "resource.html#others",
    "title": "Resources",
    "section": "Others",
    "text": "Others\n\n\n\n\n\n\n\n\n\n\nStanford CS224W: Machine Learning with Graphs\n\n\n\nNN\n\n\nBasic\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model.‚Ä¶\n\n\n\nStanford\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStanford CS 330: Deep Multi-Task and Meta Learning\n\n\n\nNN\n\n\nBasic\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model.‚Ä¶\n\n\n\nStanford\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCMU 11-777: MultiModal Machine Learning\n\n\n\nNN\n\n\nBasic\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model.‚Ä¶\n\n\n\nStanford\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStanford CS246: Mining Massive Data Sets\n\n\n\nNN\n\n\nBasic\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model.‚Ä¶\n\n\n\nStanford\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "courses/cornell-ml.html",
    "href": "courses/cornell-ml.html",
    "title": "CS4780: Machine Learning for Intelligent Systems",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "resource.html#hardware",
    "href": "resource.html#hardware",
    "title": "Resources",
    "section": "Hardware",
    "text": "Hardware\n\n\n\n\n\n\n\n\n\n\nCMU 11-714: Deep Learning Systems\n\n\n\nNN\n\n\nBasic\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model.‚Ä¶\n\n\n\nStanford\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCMU 11-868: Large Language Model Systems\n\n\n\nNN\n\n\nBasic\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model.‚Ä¶\n\n\n\nStanford\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCMU 15-849: Machine Learning Systems\n\n\n\nNN\n\n\nBasic\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model.‚Ä¶\n\n\n\nStanford\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCS 5775: Machine Learning Hardware and Systems\n\n\n\nNN\n\n\nBasic\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model.‚Ä¶\n\n\n\nStanford\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGPU Mode\n\n\n\nNN\n\n\nBasic\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model.‚Ä¶\n\n\n\nStanford\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "courses/mit-la.html",
    "href": "courses/mit-la.html",
    "title": "MIT 18.06: Linear Algebra",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/stanford-parallel-computing.html",
    "href": "courses/stanford-parallel-computing.html",
    "title": "Stanford CS149: Parallel Computing",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/mit-matrix-calculus.html",
    "href": "courses/mit-matrix-calculus.html",
    "title": "MIT 18.S096: Matrix Calculus For Machine Learning And Beyond",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/mit-calculus1.html",
    "href": "courses/mit-calculus1.html",
    "title": "MIT 18.01: Single Variable Calculus",
    "section": "",
    "text": "Course Link"
  },
  {
    "objectID": "courses/mit-calculus2.html",
    "href": "courses/mit-calculus2.html",
    "title": "MIT 18.02: Multivariable Calculus",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/ucb-intro-ai.html",
    "href": "courses/ucb-intro-ai.html",
    "title": "UCB CS188: Introduction to Artificial Intelligence ",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/stanford-intro-ai.html",
    "href": "courses/stanford-intro-ai.html",
    "title": "Stanford CS221: Artificial Intelligence: Principles and Techniques",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/mit-ds.html",
    "href": "courses/mit-ds.html",
    "title": "MIT RES.18-009: Learn Differential Equations: Up Close With Gilbert Strang And Cleve Moler",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/stanford-nlp.html",
    "href": "courses/stanford-nlp.html",
    "title": "Stanford CS224N: Natural Language Processing with Deep Learning",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "resource.html#youtube",
    "href": "resource.html#youtube",
    "title": "Resources",
    "section": "Youtube",
    "text": "Youtube\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Description\n        \n         \n          link\n        \n         \n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nDescription\n\n\nlink\n\n\n \n\n\n\n\n\n\nMIT OpenCourseWare\n\n\nMIT OpenCourseWare is a web-based publication of virtually all MIT course content. OCW is open and available to the world and is a permanent MIT activity.\n\n\nhttps://www.youtube.com/user/MIT\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "courses/ucb-cs61a.html",
    "href": "courses/ucb-cs61a.html",
    "title": "UCB CS 61A: Structure and Interpretation of Computer Programs",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/cmu-os.html",
    "href": "courses/cmu-os.html",
    "title": "CMU 15-213: Introduction to Computer Systems",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "courses/stanford-massive.html",
    "href": "courses/stanford-massive.html",
    "title": "Stanford CS246: Mining Massive Data Sets",
    "section": "",
    "text": "course"
  },
  {
    "objectID": "resource.html#youtuber",
    "href": "resource.html#youtuber",
    "title": "Resources",
    "section": "Youtuber",
    "text": "Youtuber\n\n\n\n\n\n\n\n\n\n\nSteve Brunton\n\n\n\n\n\nMIT OpenCourseWare is a web-based publication of virtually all MIT course content. OCW is open and available to the world and is a permanent MIT activity.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAndrej Karpathy\n\n\n\n\n\nMIT OpenCourseWare is a web-based publication of virtually all MIT course content. OCW is open and available to the world and is a permanent MIT activity.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3Blue1Brown\n\n\n\n\n\nMIT OpenCourseWare is a web-based publication of virtually all MIT course content. OCW is open and available to the world and is a permanent MIT activity.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatQuest with Josh Starmer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLex Fridman\n\n\n\n\n\nMIT OpenCourseWare is a web-based publication of virtually all MIT course content. OCW is open and available to the world and is a permanent MIT activity.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resource.html#blogs",
    "href": "resource.html#blogs",
    "title": "Resources",
    "section": "Blogs",
    "text": "Blogs\n\n\n\n\n\n\n\n\n\n\nLil‚ÄôLog\n\n\n\n\n\nMIT OpenCourseWare is a web-based publication of virtually all MIT course content. OCW is open and available to the world and is a permanent MIT activity.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Real-World ML Blog\n\n\n\n\n\nMIT OpenCourseWare is a web-based publication of virtually all MIT course content. OCW is open and available to the world and is a permanent MIT activity.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHugging Face Daily Paper\n\n\n\n\n\nMIT OpenCourseWare is a web-based publication of virtually all MIT course content. OCW is open and available to the world and is a permanent MIT activity.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resource.html#books",
    "href": "resource.html#books",
    "title": "Resources",
    "section": "Books",
    "text": "Books\n\n\n\n\n\n\n\n\n\n\nPattern Recognition and Machine Learning\n\n\n\n\n\nMIT OpenCourseWare is a web-based publication of virtually all MIT course content. OCW is open and available to the world and is a permanent MIT activity.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning: Foundations and Concepts\n\n\n\n\n\nMIT OpenCourseWare is a web-based publication of virtually all MIT course content. OCW is open and available to the world and is a permanent MIT activity.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbabilistic Machine Learning: An Introduction\n\n\n\n\n\nMIT OpenCourseWare is a web-based publication of virtually all MIT course content. OCW is open and available to the world and is a permanent MIT activity.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbabilistic Machine Learning: Advanced Topics\n\n\n\n\n\nMIT OpenCourseWare is a web-based publication of virtually all MIT course content. OCW is open and available to the world and is a permanent MIT activity.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMathematics for Machine Learning\n\n\n\n\n\nMIT OpenCourseWare is a web-based publication of virtually all MIT course content. OCW is open and available to the world and is a permanent MIT activity.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArtificial Intelligence: A Modern Approach\n\n\n\n\n\nMIT OpenCourseWare is a web-based publication of virtually all MIT course content. OCW is open and available to the world and is a permanent MIT activity.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Algebra and Optimization for Machine Learning: A Textbook\n\n\n\n\n\nMIT OpenCourseWare is a web-based publication of virtually all MIT course content. OCW is open and available to the world and is a permanent MIT activity.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics for High-Dimensional Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "courses/mit-calculus1.html#why-this-course",
    "href": "courses/mit-calculus1.html#why-this-course",
    "title": "MIT 18.01: Single Variable Calculus",
    "section": "Why this course?",
    "text": "Why this course?\n\nSingle Variable Calculus is one of the fundamental building blocks of AI and machine learning. Many key concepts in AI, such as gradient descent, optimization, and backpropagation, heavily rely on calculus principles.\nEssential for Understanding Optimization Algorithms\n\nMachine learning models are trained using optimization algorithms, which minimize loss functions.\nGradient Descent, the most widely used optimization technique, is based on derivatives to adjust model parameters.\nConvex functions and critical points help in finding optimal solutions.\n\nIntegral Calculus in Probabilistic Models\n\nMany AI models use probability distributions (e.g., Gaussian, Poisson, Exponential).\nIntegrals are needed to compute expectations, probabilities, and marginal distributions, essential in Bayesian statistics and deep learning.\n\nFoundations for Neural Networks & Deep Learning ‚Ä¢ Activation functions (e.g., Sigmoid, ReLU, Softmax) require differentiation. ‚Ä¢ Backpropagation, the core algorithm in neural networks, relies on chain rule differentiation to update weights."
  },
  {
    "objectID": "courses/mit-calculus2.html#why-this-course",
    "href": "courses/mit-calculus2.html#why-this-course",
    "title": "MIT 18.02: Multivariable Calculus",
    "section": "Why this course?",
    "text": "Why this course?\n\nEssential for Machine Learning & Deep Learning ‚Ä¢ Many cost functions in deep learning involve multiple variables, requiring partial derivatives for optimization. ‚Ä¢ Gradient Descent in Higher Dimensions ‚Äì In neural networks, weights are updated using gradients in multi-dimensional space. ‚Ä¢ Hessian Matrices & Second-Order Optimization ‚Äì Used in advanced optimization algorithms like Newton‚Äôs method.\nFoundation for Probabilistic Machine Learning & Statistics ‚Ä¢ Multivariate Probability Distributions ‚Äì Needed for Gaussian Mixture Models, Bayesian Inference, and Principal Component Analysis (PCA). ‚Ä¢ Expectation & Variance in Multiple Dimensions ‚Äì Important for probabilistic AI models."
  },
  {
    "objectID": "courses/mit-la.html#why-this-course",
    "href": "courses/mit-la.html#why-this-course",
    "title": "MIT 18.06: Linear Algebra",
    "section": "Why this course?",
    "text": "Why this course?\nAll operation in the world of AI is based on linear algebra. This course is a must for anyone who wants to understand the math behind AI.\n\nCore Foundation for Machine Learning and Deep Learning\n\nMachine learning models rely on matrix operations for training and prediction.\n\nNeural networks use weight matrices and activations that require linear transformations.\n\nGradient-based optimization techniques such as gradient descent involve matrix calculus.\n\nEssential for Dimensionality Reduction and Feature Engineering\n\nPrincipal Component Analysis (PCA) and Singular Value Decomposition (SVD) are used in data compression and feature extraction.\n\nLatent Semantic Analysis (LSA) in Natural Language Processing (NLP) relies on matrix factorization.\n\nOptimization and AI Model Training\n\nLeast Squares Regression and ridge regression are direct applications of orthogonal projections in vector spaces.\n\nMatrix calculus is necessary for computing gradients and optimizing model performance."
  },
  {
    "objectID": "courses/mit-matrix-calculus.html#why-study-matrix-calculus-for-machine-learning",
    "href": "courses/mit-matrix-calculus.html#why-study-matrix-calculus-for-machine-learning",
    "title": "MIT 18.S096: Matrix Calculus For Machine Learning And Beyond",
    "section": "Why Study Matrix Calculus for Machine Learning?",
    "text": "Why Study Matrix Calculus for Machine Learning?\n\nEssential for Understanding Deep Learning\n\nBackpropagation, the core algorithm for training deep neural networks, relies on matrix differentiation.\n\nComputing gradients efficiently is necessary for updating model parameters using gradient descent.\n\nCrucial for Optimization and Training AI Models\n\nOptimization algorithms such as SGD, Adam, and Newton‚Äôs method require an understanding of matrix gradients.\n\nHessian matrices improve convergence rates in second-order optimization techniques.\n\nFoundational for Probabilistic Models and Variational Inference\n\nMany probabilistic models involve matrix calculus for deriving gradients of likelihood functions.\n\nGaussian Processes, Bayesian Neural Networks, and Expectation-Maximization (EM) algorithms rely on matrix derivatives.\n\nKey for Natural Language Processing and Transformer Models\n\nWord embeddings, self-attention mechanisms, and transformer architectures depend on efficient computation of matrix derivatives.\n\nUnderstanding Jacobian and Hessian computations helps in designing stable and efficient NLP models.\n\nApplied in Reinforcement Learning and Robotics\n\nPolicy gradients in reinforcement learning require differentiation of expectation functions.\n\nControl systems and robotics involve matrix calculus for trajectory optimization and system modeling."
  },
  {
    "objectID": "courses/mit-matrix-method.html#why-study-matrix-methods-for-data-analysis-and-machine-learning",
    "href": "courses/mit-matrix-method.html#why-study-matrix-methods-for-data-analysis-and-machine-learning",
    "title": "MIT 18.065: Matrix Methods in Data Analysis, Signal Processing, and Machine Learning",
    "section": "Why Study Matrix Methods for Data Analysis and Machine Learning?",
    "text": "Why Study Matrix Methods for Data Analysis and Machine Learning?\n\nEssential for High-Dimensional Data Analysis\n\nPrincipal Component Analysis (PCA) and SVD help in reducing dimensionality and extracting meaningful insights from large datasets.\n\nEigenvalue decomposition and low-rank approximations are widely used in exploratory data analysis.\n\nCrucial for Machine Learning and AI Optimization\n\n\n\nLeast squares regression is a fundamental method in supervised learning and optimization.\n\nMany ML algorithms, including kernel methods, support vector machines, and deep learning architectures, rely on matrix factorization techniques.\n\n\nKey for Signal Processing and Image Recognition\n\nFourier transforms and wavelets are used in speech processing, medical imaging, and object recognition.\n\nLow-rank matrix approximations improve efficiency in compressing high-dimensional signals.\n\nApplied in Graph Learning and Network Analysis\n\nSpectral clustering and graph embeddings use matrix methods to analyze social networks and biological datasets.\n\nEigenvalues and adjacency matrices play a role in recommendation systems, fraud detection, and NLP applications.\n\nSupports Scalable and Computationally Efficient AI Systems\n\nMany big data applications rely on fast matrix decompositions for computational efficiency.\n\nOptimization in neural networks is enhanced by understanding matrix methods for large-scale training."
  }
]